{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# EDGAR - Reading Information Tables in Text Format - Advanced Text Mining (25 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### So far, we have collected CIKs for each of the Mutual Funds, then looked up the links of all the 13F HRs and the Information Tables, and identified them either text tables or xml tables. In class, we  obtained the information from the xml tables.  In this assignment we will obtain information from the text file.  These are not as nicely structured as the xml output.   In a csv (HW_Mutual_Fund_Info_Table_Link.csv) you will find a few of these links.  Your goal for this part of the homework is to obtain the link to the text files from the attached file  (HW_Mutual_Fund_Info_Table_Link.csv).  Then you will write a code that will go to the links of all the linked text files, and extract some columns.  Do not use Beutiful Soup for this assignment.   We provide some initial code to guide your initial steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### First we have to collect the links of the text Info Tables in lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import bs4 as bs\n",
    "from bs4 import SoupStrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "CIK = []\n",
    "xml_link = []\n",
    "xml_Name = []\n",
    "xml_date = []\n",
    "\n",
    "text_link = []\n",
    "text_Name = []\n",
    "text_date = []\n",
    "\n",
    "input_file = open('HW_Mutual_Fund_Info_Table_Link.csv', 'r')\n",
    "rows = input_file.readlines()\n",
    "input_file.close()\n",
    "\n",
    "# Your Code on Enumerating the Lists.  The result should be three lists, text_link, text_Name,\n",
    "#and text_date.  Each should have length 122.\n",
    "\n",
    "for row in range(1, len(rows)):\n",
    "    column = rows[row].split(\",\")\n",
    "    if column[4] == \"xml\":\n",
    "        xml_link.append(column[3])\n",
    "        xml_Name.append(column[1])\n",
    "        xml_date.append(column[2])\n",
    "    else:\n",
    "        text_link.append(column[3])\n",
    "        text_Name.append(column[1])\n",
    "        text_date.append(column[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Keep only the links that correspond to a date after 2010 (Don't inlude 2010, start at 2011).  Hint: you can use the datetime library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "#Use the following list to store the filtered values.\n",
    "filtered_dates = []\n",
    "filtered_name = []\n",
    "filtered_link = []\n",
    "\n",
    "filtered_text_date = []\n",
    "filtered_text_name = []\n",
    "filtered_text_link = []\n",
    "\n",
    "\n",
    "\n",
    "#Enter code here to to keep only the dates corresponding to after 2010. \n",
    "\n",
    "xml_date_converted = [dt.strptime(x,'%Y/%m/%d') for x in xml_date]\n",
    "xml_year = [x.year for x in xml_date_converted]\n",
    "for pos in range(0, len(xml_year)):\n",
    "    if xml_year[pos] > 2010:\n",
    "        filtered_dates.append(xml_date[pos])\n",
    "        filtered_name.append(xml_Name[pos])\n",
    "        filtered_link.append(xml_link[pos])\n",
    "\n",
    "text_date_converted = [dt.strptime(x,'%Y/%m/%d') for x in text_date]\n",
    "text_year = [x.year for x in text_date_converted]  \n",
    "for pos in range(0, len(text_year)):\n",
    "    if text_year[pos] > 2010:\n",
    "        filtered_text_date.append(text_date[pos])\n",
    "        filtered_text_name.append(text_Name[pos])\n",
    "        filtered_text_link.append(text_link[pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Your filtered list should now have 8 elements.  These represent 3 mutual funds.   The first one has CIK=1311981 (from the link you can find this at data/1311981 ).  The second was has CIK 813470.  The third one has CIK 1432353. \n",
    "\n",
    "['https://www.sec.gov/Archives/edgar/data/1311981/000116204413000513/0001162044-13-000513.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347013000006/0000813470-13-000006.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347013000001/0000813470-13-000001.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347012000023/0000813470-12-000023.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347012000019/0000813470-12-000019.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347012000014/0000813470-12-000014.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347012000003/0000813470-12-000003.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/1432353/000114420411008428/0001144204-11-008428.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Next, for each text link, extract the name of issuer, CUSIP, and the Quantity of shares.  You will also want to keep track of the mutual fund name as well as the filing report date.  \n",
    "\n",
    "#### Your output file should have 5 columns.  The first is the issue date of the form which can be found in the filtered_date list (this will be repeated for the same form).  The second is the mutual fund name which can be found in the filtered_name list (this will be repeated).  The third, fourth and fifith are the name of issuer, CUSIP, and shares respectively.  Make sure to account for the fact that while some of the text files have the same formatting, others do not.  This means you will have to look through them to make sure your code works for the each text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert filename to URL links\n",
    "urls =[]\n",
    "for link in filtered_link:\n",
    "    urls.append(\"https://www.sec.gov\" + link)\n",
    "\n",
    "\n",
    "url_t = []\n",
    "for t_link in filtered_text_link:\n",
    "    url_t.append(\"https://www.sec.gov\" + t_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_fund_name = []\n",
    "info_date = []\n",
    "name_of_issuer = []\n",
    "CUSIP = []\n",
    "SHRS_or_PRN = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(0,len(urls)):\n",
    "        source = urllib.request.urlopen(urls[ind])\n",
    "        html = source.read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html, \"html5lib\")\n",
    "        Info = soup.find_all(\"table\", summary = \"Form 13F-NT Header Information\")\n",
    "        \n",
    "        for row in Info[0].find_all('tr')[3:]:\n",
    "            col = row.find_all('td')\n",
    "            mutual_fund_name.append(filtered_name[ind])\n",
    "            info_date.append(filtered_dates[ind])\n",
    "            name_of_issuer.append(col[0].getText())\n",
    "            CUSIP.append(col[2].getText())\n",
    "            SHRS_or_PRN.append(col[4].getText())\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for indx, vl in enumerate(url_t):\n",
    "    Input = urllib.request.urlopen(url_t[indx]).read().decode('utf-8')\n",
    "    beg = Input.upper().find('NAME OF ISSUER')\n",
    "    end = Input.find('</TABLE')\n",
    "    t = Input[beg:end].split('\\n')\n",
    "    \n",
    "    \n",
    "    for row in t:\n",
    "        \n",
    "        row = row.split(\"  \")\n",
    "        row = [x for x in row if x]\n",
    "  \n",
    "        if (len(row) > 4):\n",
    "        \n",
    "            row[2] = ''.join(c for c in row[2] if c.isalnum())\n",
    "            row[4] = ''.join(c for c in row[4] if c.isdigit())\n",
    "\n",
    "            \n",
    "            df = pd.DataFrame(tuples)\n",
    "            \n",
    "            mutual_fund_name.append(filtered_text_name[indx])\n",
    "            info_date.append(filtered_text_date[indx])\n",
    "\n",
    "            name_of_issuer.append(row[0])\n",
    "            CUSIP.append(row[2])\n",
    "            SHRS_or_PRN.append(row[4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Finally, you write down the lists in the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "d = [mutual_fund_name, info_date, name_of_issuer, CUSIP, SHRS_or_PRN]\n",
    "edgar_file = zip_longest(*d, fillvalue = '')\n",
    "with open('edgar_file.csv', 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow((\"Mutual_Fund_Name\", \"Info_Date\", \"Name_of_Issuer\", \"CUSIP\", \"Qty\"))\n",
    "    wr.writerows(edgar_file)\n",
    "\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
